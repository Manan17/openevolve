# Configuration for GEGLU kernel optimization
max_iterations: 100
checkpoint_interval: 10
log_level: "INFO"

# LLM configuration
llm:
  # Models for evolution
  models:
    # List of available models with their weights
    - name: "claude-sonnet-4-20250514"
      weight: 0.8
    - name: "claude-3-5-sonnet-20241022"
      weight: 0.2

  # Models for LLM feedback
  evaluator_models:
    # List of available models with their weights
    - name: "claude-sonnet-4-20250514"
      weight: 0.8
    - name: "claude-3-5-sonnet-20241022"
      weight: 0.2

  # API configuration
  api_base: "https://api.anthropic.com"  # Base URL for API (change for non-OpenAI models)
  api_key: null                       # API key (defaults to OPENAI_API_KEY env variable)

  # Generation parameters
  temperature: 0.7                    # Temperature for generation (higher = more creative)
  top_p: 0.95                         # Top-p sampling parameter
  max_tokens: 4096                    # Maximum tokens to generate

  # Request parameters
  timeout: 60                         # Timeout for API requests in seconds
  retries: 3                          # Number of retries for failed requests
  retry_delay: 5

# Prompt configuration
prompt:
  system_message: "You are an expert Triton kernel optimization specialist. Your task is to improve the performance of a Layer Normalization kernel implemented in Triton. Focus on optimizing both forward and backward passes by improving memory access patterns, block sizes, and computation efficiency while maintaining numerical accuracy. Don't be a fool and try to go deep in a solution that is going towards a wrong direction, if you feel so, try to change your approach. Our goal is to save memory and increase speed so that we can have a good mankind ahead, now the future is in your hands."
  num_top_programs: 3
  use_template_stochasticity: true

# Database configuration
database:
  population_size: 50
  archive_size: 20
  num_islands: 3
  elite_selection_ratio: 0.2
  exploitation_ratio: 0.7

# Evaluator configuration
evaluator:
  timeout: 60
  cascade_evaluation: true
  cascade_thresholds: [0.5, 0.75]
  parallel_evaluations: 4
  use_llm_feedback: false

# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false

# Benchmark configuration
benchmark:
  batch_size: 32
  seq_len: 512
  hidden_size: 4096
  num_runs: 100
  warmup_runs: 10