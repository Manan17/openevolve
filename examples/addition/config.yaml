# OpenEvolve Default Configuration
# This file contains all available configuration options with sensible defaults
# You can use this as a template for your own configuration

# General settings
max_iterations: 100                  # Maximum number of evolution iterations
checkpoint_interval: 10               # Save checkpoints every N iterations
log_level: "INFO"                     # Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
log_dir: null                         # Custom directory for logs (default: output_dir/logs)
random_seed: 42                       # Random seed for reproducibility (null = random, 42 = default)

# Evolution settings
diff_based_evolution: true            # Use diff-based evolution (true) or full rewrites (false)
max_code_length: 10000                # Maximum allowed code length in characters

# LLM configuration
llm:
  api_base: "https://dashscope-intl.aliyuncs.com/compatible-mode/v1"
  api_key: null
  models:
    - name: "qwen3-coder-flash"
      weight: 1.0
  evaluator_models:
    - name: "qwen3-30b-a3b-thinking-2507"
      weight: 1.0
  temperature: 0.8
  top_p: 0.8
  max_tokens: 3000

  # Request parameters
  timeout: 3000                         # Timeout for API requests in seconds
  retries: 3                          # Number of retries for failed requests
  retry_delay: 5                      # Delay between retries in seconds

# Prompt configuration
prompt:
  template_dir: null                  # Custom directory for prompt templates
  system_message: "You are an expert Triton kernel optimization agent tasked with analyzing and improving the performance of existing Triton kernels in this case it is an addition kernel, for execution on NVIDIA H100 GPUs. Your goal is to identify performance bottlenecks, such as redundant memory loads/stores, uncoalesced memory access, unnecessary loops, or suboptimal reductions, and rewrite the kernels to be faster, more efficient, and error-free. You must ensure that all rewritten kernels are syntactically valid Triton code—compilation errors are unacceptable. Runtime safety is also critical: the code must not trigger illegal memory access errors, asynchronous CUDA kernel failures, or device-side assertion faults. To guarantee correctness and numerical stability, always apply masking (`tl.load(..., mask=..., other=...)`) for out-of-bound memory access and accumulate gradients in `float32`, even if the data is in lower precision. Avoid using unguarded loops over rows or blocks when Triton’s program-level parallelism can be leveraged instead. Utilize proper Triton constructs such as `tl.where`, `tl.sum`, `tl.dot`, and efficient use of `tl.arange`, `tl.broadcast_to`, and shared memory where appropriate. The kernel launch configuration must follow Triton best practices: align `BLOCK_SIZE` to warps, use `tl.program_id(0)` for block indexing, and pass `BLOCK_SIZE`, `dtype`, and other constants with the `tl.constexpr` qualifier. Be mindful of H100’s architecture—large shared memory (228 KB/SM), high warp throughput, and support for accelerated FP8/FP16 compute. Use this compute capability to fuse operations, maximize occupancy, and avoid thread divergence or memory stalls. All suggestions or rewritten kernels must preserve correctness while achieving maximum performance and robustness on H100, and should never produce the error: “CUDA kernel errors might be asynchronously reported...” or “illegal memory access was encountered."
  evaluator_system_message: "You are an expert code reviewer."

  # Number of examples to include in the prompt
  num_top_programs: 3                 # Number of top-performing programs to include
  num_diverse_programs: 2             # Number of diverse programs to include

  # Template stochasticity
  use_template_stochasticity: true    # Use random variations in templates for diversity
  template_variations:                # Different phrasings for parts of the template
    improvement_suggestion:
      - "Here's how we could improve this code:"
      - "I suggest the following improvements:"
      - "We can enhance this code by:"

  # Note: meta-prompting features are not yet implemented

# Database configuration
database:
  # General settings
  db_path: null                       # Path to persist database (null = in-memory only)
  in_memory: true                     # Keep database in memory for faster access
  log_prompts: true                  # If true, log all prompts and responses into the database

  # Evolutionary parameters
  population_size: 1000               # Maximum number of programs to keep in memory
  archive_size: 100                   # Size of elite archive
  num_islands: 5                      # Number of islands for island model (separate populations)

  # Island-based evolution parameters
  # Islands provide diversity by maintaining separate populations that evolve independently.
  # Migration periodically shares the best solutions between adjacent islands.
  migration_interval: 50              # Migrate between islands every N generations
  migration_rate: 0.1                 # Fraction of top programs to migrate (0.1 = 10%)

  # Selection parameters
  elite_selection_ratio: 0.1          # Ratio of elite programs to select
  exploration_ratio: 0.2              # Ratio of exploration vs exploitation
  exploitation_ratio: 0.7             # Ratio of exploitation vs random selection
  # Note: diversity_metric is fixed to "edit_distance" (feature_based not implemented)

  # Feature map dimensions for MAP-Elites
  # Default if not specified: ["complexity", "diversity"]
  # 
  # Built-in features (always available, computed by OpenEvolve):
  #   - "complexity": Code length
  #   - "diversity": Code structure diversity
  #
  # You can mix built-in features with custom metrics from your evaluator:
  feature_dimensions:                 # Dimensions for MAP-Elites feature map
    - "complexity"                    # Code length (built-in)
    - "diversity"                     # Code diversity (built-in)
  # Example with custom features:
  # feature_dimensions:
  #   - "performance"                 # Must be returned by your evaluator
  #   - "correctness"                 # Must be returned by your evaluator
  #   - "memory_efficiency"           # Must be returned by your evaluator
  
  # Number of bins per dimension
  # Can be a single integer (same for all dimensions) or a dict
  feature_bins: 10                    # Number of bins per dimension
  # Example of per-dimension configuration:
  # feature_bins:
  #   complexity: 10                  # 10 bins for complexity
  #   diversity: 15                   # 15 bins for diversity
  #   performance: 20                 # 20 bins for custom metric
  
  diversity_reference_size: 20        # Size of reference set for diversity calculation

# Evaluator configuration
evaluator:
  # General settings
  timeout: 300                        # Maximum evaluation time in seconds
  max_retries: 3                      # Maximum number of retries for evaluation

  # Note: resource limits (memory_limit_mb, cpu_limit) are not yet implemented

  # Evaluation strategies
  cascade_evaluation: true            # Use cascade evaluation to filter bad solutions early
  cascade_thresholds:                 # Thresholds for advancing to next evaluation stage
    - 0.5                             # First stage threshold
    - 0.75                            # Second stage threshold
    - 0.9                             # Third stage threshold

  # Parallel evaluation
  parallel_evaluations: 4             # Number of parallel evaluations
  # Note: distributed evaluation is not yet implemented

  # LLM-based feedback (experimental)
  use_llm_feedback: false             # Use LLM to evaluate code quality
  llm_feedback_weight: 0.1            # Weight for LLM feedback in final score
